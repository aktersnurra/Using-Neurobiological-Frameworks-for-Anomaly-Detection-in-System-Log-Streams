@book{Hawkins:2004:INT:993636,
 author = {Hawkins, Jeff and Blakeslee, Sandra},
 title = {On Intelligence},
 year = {2004},
 isbn = {0805074562},
 publisher = {Times Books},
 address = {New York, NY, USA},
} 

@online{Intelligence, 
    author = {Caleb Garling},
    title = {Jeff Hawkins on Firing Up the Silicon Brain},
    year = 2015,
    url = {https://www.wired.com/brandlab/2015/05/jeff-hawkins-firing-silicon-brain/},
    urldate = {2015-07-07}
}

@online{Hinton, 
    author = {Steve LeVine},
    title = {Artificial intelligence pioneer says we need to start over},
    year = 2017,
    url = {https://www.axios.com/artificial-intelligence-pioneer-says-we-need-to-start-over-1513305524-f619efbd-9db0-4947-a9b2-7a4c310a28fe.html},
}


@book{Allocortex,
keywords = {Cerebral cortex; Cytoarchitectonics; Telencephalon; Electronic books},
series = {Studies of Brain Function, 4},
abstract = {This is a timely opus. Most of us now are too young to remember the unpleasant ring of a polemic between those who produced "hair-splitting" parcellations of the cortex (to paraphrase one of O. Vogt's favourite expressions) and those who saw the cortex as a homogeneous matrix sus­ taining the reverberations of EEG waves (to paraphrase Bailey and von Bonin). One camp accused the other of producing bogus preparations with a paint brush, and the other way around the accusation was that of poor eye-sight. Artefacts of various sorts were invoked to explain the opponent's error, ranging from perceptual effects (Mach bands crispening the areal borders) to poor fixation supposedly due to perfusion too soon (!) after death. I have heard most of this directly from the protagonists' mouths. The polemic was not resolved but it has mellowed with age and ultimately faded out. I was relieved to see that Professor Braak elegantly avoids dis­ cussion of an extrememist tenet, that of "hair-sharp" areal boundaries, which makes little sense in developmental biology and is irrelevant to neurophysiology. It was actually detrimental to cortical neuroanatomy, since its negation led to the idea that structurally distinct areas are not at all existent. Yet, nobody would deny the reality of five fingers on one hand even if the detailed assignment of every epidermal cell to one finger or another is obviously impossible.},
isbn = {3-642-81522-7},
year = {1980},
title = {Architectonics of the Human Telencephalic Cortex},
language = {eng},
author = {Braak, Heiko},
}



@incollection{Neocortex,
pages = {291--303},
booktitle = {Encyclopedia of the Human Brain, Four-Volume Set},
isbn = {978-0-08-054803-6},
title = {Neocortex},
language = {eng},
author = {Kaas, Jon H.},
}

@article{LodatoSimona2015GNDi,
keywords = {neocortex ; identity ; projection neurons ; interneurons ; cortical networks ; myelination profile},
month = {11},
issn = {1081-0706},
abstract = {The neocortex is the part of the brain responsible for execution of higher-order brain functions, including cognition, sensory perception, and sophisticated motor control. During evolution, the neocortex has developed an unparalleled neuronal diversity, which still remains partly unclassified and unmapped at the functional level. Here, we broadly review the structural blueprint of the neocortex and discuss the current classification of its neuronal diversity. We then cover the principles and mechanisms that build neuronal diversity during cortical development and consider the impact of neuronal class-specific identity in shaping cortical connectivity and function.},
journal = {Annual Review of Cell and Developmental Biology},
pages = {699--720},
volume = {31},
number = {1},
year = {2015},
title = {Generating Neuronal Diversity in the Mammalian Cerebral Cortex},
language = {eng},
author = {Lodato, Simona and Arlotta, Paola},
}

@article{folded,
keywords = {Biological Evolution; Neocortex -- Physiology;},
issn = {0092-8674},
abstract = {The size and surface area of the mammalian brain are thought to be critical determinants of intellectual ability. Recent studies show that development of the gyrated human neocortex involves a lineage of neural stem and transit-amplifying cells that forms the outer subventricular zone (OSVZ), a proliferative region outside the ventricular epithelium. We discuss how proliferation of cells within the OSVZ expands the neocortex by increasing neuron number and modifying the trajectory of migrating neurons. Relating these features to other mammalian species and known molecular regulators of the mouse neocortex suggests how this developmental process could have emerged in evolution.},
journal = {Cell},
pages = {18--36},
volume = {146},
publisher = {Elsevier Inc.},
number = {1},
year = {2011},
title = {Development and Evolution of the Human Neocortex},
language = {eng},
author = {Lui, Jan h. and Hansen, David v. and Kriegstein, Arnold r.},
}

@article{MountcastleVernon1997,
keywords = {Cortex ; Brain Architecture ; Neuroanatomy, Histology & Cytology},
month = {4},
url = {http://search.proquest.com/docview/20611728/},
issn = {0006-8950},
journal = {Brain},
pages = {701--701},
volume = {120},
number = {4},
year = {1997},
title = {The columnar organization of the neocortex},
language = {eng},
author = {Mountcastle, Vernon},
}



@article{MajorGuy2013APoN,
keywords = {spike ; action potential ; NMDA receptor ; NMDAR ; synaptic integration ; computational subunit},
month = {78},
issn = {0147-006x},
abstract = {Dendrites are the main recipients of synaptic inputs and are important sites that determine neurons' input-output functions. This review focuses on thin neocortical dendrites, which receive the vast majority of synaptic inputs in cortex but also have specialized electrogenic properties. We present a simplified working-model biophysical scheme of pyramidal neurons that attempts to capture the essence of their dendritic function, including the ability to behave under plausible conditions as dynamic computational subunits. We emphasize the electrogenic capabilities of NMDA receptors (NMDARs) because these transmitter-gated channels seem to provide the major nonlinear depolarizing drive in thin dendrites, even allowing full-blown NMDA spikes. We show how apparent discrepancies in experimental findings can be reconciled and discuss the current status of dendritic spikes in vivo; a dominant NMDAR contribution would indicate that the input-output relations of thin dendrites are dynamically set by network activity and cannot be fully predicted by purely reductionist approaches.},
journal = {Annual Review of Neuroscience},
pages = {1--24},
volume = {36},
year = {2013},
title = {Active Properties of Neocortical Pyramidal Neuron Dendrites},
language = {eng},
author = {Major, Guy and Larkum, Matthew E. and Schiller, Jackie},
}


@Book{IVIlayers,
title = { Basic anatomy },
author = { Mitchell, G. A. G. (George Archibald Grant) and Patterson, E. L },
publisher = { Edinburgh : E. \& S. Livingstone },
year = { 1954 },
type = { Book; Book/Illustrated },
subjects = { Anatomy; Human anatomy },
language = { English },
}


@book{crossman2014neuroanatomy,
  title={Neuroanatomy E-Book: An Illustrated Colour Text},
  author={Crossman, A.R. and Neary, D.},
  isbn={9780702 054068},
  lccn={13047336},
  series={Illustrated Colour Text},
  year={2014},
  publisher={Elsevier Health Sciences}
}


@article{PyramidalNeuron,
issn = {1941-6016},
journal = {Scholarpedia},
volume = {4},
number = {5},
year = {2009},
title = {Pyramidal neuron},
language = {eng},
author = {Spruston, Nelson},
}

@book{BrainSketch,
url = {http://hdl.handle.net/2027/hvd.hc4bm2},
publisher = {Holt.},
year = {1886},
title = {The human body: an elementary text-book of anatomy, physiology, and hygiene : including a special account of the action upon the body of alcohol and other stimulants and narcotics},
author = {Martin, H. Newell},
}


@article{doi:10.1093/cercor/13.1.2,
author = {Mountcastle, Vernon B.},
title = {Introduction},
journal = {Cerebral Cortex},
volume = {13},
number = {1},
pages = {2-4},
year = {2003},
doi = {10.1093/cercor/13.1.2},
URL = {http://dx.doi.org/10.1093/cercor/13.1.2},
eprint = {}
}


@misc{nupic,
  author       = {Matthew Taylor and
                  Scott Purdy and
                  breznak and
                  Chetan Surpur and
                  Austin Marshall and
                  David Ragazzi and
                  Subutai Ahmad and
                  numenta-ci and
                  Andrew Malta and
                  vitaly-krugl and
                  Pascal Weinberger and
                  Akhila and
                  Marcus Lewis and
                  Richard Crowder and
                  Marion Le Borgne and
                  Yuwei and
                  Christopher Simons and
                  Ryan J. McCall and
                  Mihail Eric and
                  Utensil Song and
                  keithcom and
                  Nathanael Romano and
                  Sagan Bolliger and
                  hernandezurbina and
                  James Bridgewater and
                  Ian Danforth and
                  Jared Weiss and
                  Tom Silver and
                  David Ray and
                  Luiz Scheinkman},
  title        = {numenta/nupic: 1.0.3},
  month        = {1},
  year         = {2018},
  url          = {https://github.com/numenta/nupic}
}

@unpublished{bami,
author={Hawkins, J. and
        Ahmad, S. and
        Purdy, S. and
        Lavin, A.},
title={Biological and Machine Intelligence (BAMI)},
note={Initial online release 0.4},
url={http://www.numenta.com/biological-and-machine-intelligence/},
year={2016}
}



@online{cortical.io,
        url = {http://www.cortical.io/use-cases.html},
        title = {Use Cases},
        day = {22},
        month = {1},
        year = {2018}
}

@online{semantic,
        url = {http://www.cortical.io/static/downloads/semantic-folding-theory-white-paper.pdf},
        title = {Semantic Folding: Theory and its Application in Semantic Fingerprinting},
        month = {3},
        year = {2016}
}


@article{image,
  author    = {Sara Sabour and
               Nicholas Frosst and
               Geoffrey E. Hinton},
  title     = {Dynamic Routing Between Capsules},
  journal   = {CoRR},
  volume    = {abs/1710.09829},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.09829},
  archivePrefix = {arXiv},
  eprint    = {1710.09829},
  timestamp = {Thu, 02 Nov 2017 14:25:36 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-09829},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{speech,
  author    = {Kyu J. Han and
               Akshay Chandrashekaran and
               Jungsuk Kim and
               Ian R. Lane},
  title     = {The {CAPIO} 2017 Conversational Speech Recognition System},
  journal   = {CoRR},
  volume    = {abs/1801.00059},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.00059},
  archivePrefix = {arXiv},
  eprint    = {1801.00059},
  timestamp = {Thu, 01 Feb 2018 19:52:26 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-00059},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{text,
  author    = {Yang Liu and
               Mirella Lapata},
  title     = {Learning Structured Text Representations},
  journal   = {CoRR},
  volume    = {abs/1705.09207},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.09207},
  archivePrefix = {arXiv},
  eprint    = {1705.09207},
  timestamp = {Wed, 07 Jun 2017 14:41:46 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LiuL17d},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





@article{htmvsLSTM,
author = {Yuwei Cui and Subutai Ahmad and Jeff Hawkins},
title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
journal = {Neural Computation},
volume = {28},
number = {11},
pages = {2474-2504},
year = {2016},
doi = {10.1162/NECO\_a\_00893},
    note ={PMID: 27626963},

URL = { 
        https://doi.org/10.1162/NECO_a_00893
    
},
eprint = { 
        https://doi.org/10.1162/NECO_a_00893
    
}
,
    abstract = { The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams. }
}


@article{anomaly_detection,
  author    = {Alexander Lavin and
               Subutai Ahmad},
  title     = {Evaluating Real-time Anomaly Detection Algorithms - the Numenta Anomaly
               Benchmark},
  journal   = {CoRR},
  volume    = {abs/1510.03336},
  year      = {2015},
  url       = {http://arxiv.org/abs/1510.03336},
  archivePrefix = {arXiv},
  eprint    = {1510.03336},
  timestamp = {Wed, 07 Jun 2017 14:42:49 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/LavinA15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@online{numentablog,
    author    = {Christy Maver},
    title = {Navigating Numenta’s Brain Theory through a Progression of Papers},
    year = {2018},
    month = {1},
    day = {8},
    url = {https://numenta.com/blog/2018/01/08/navigating-numenta-through-progression-of-papers/}

}

@online{numentaforum,
    author    = {Matt Taylor},
    title = {Why isn’t HTM mainstream yet},
    year = {2017},
    month = {3},
    url = {https://discourse.numenta.org/t/why-isnt-htm-mainstream-yet/2007}

}

@book{Gerstner:2014:NDS:2635959,
 author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
 title = {Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition},
 year = {2014},
 isbn = {1107635195, 9781107635197},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
} 

@online{linnaeus,
    title = {Linnaeus - Fault Detection and Classification Service},
    author = {Armin Catovic},
    year = {2017},
    month = {10},
    day = {23},
}



@article{CapaldiE.John1992TOOB,
keywords = {Investments -- Psychological Aspects ; Extinction (Psychology) -- Economic Aspects ; Human Behavior -- Psychological Aspects ; Decision Making -- Psychological Aspects;},
month = {9},
issn = {0021-8855},
journal = {Journal of Applied Behavior Analysis},
pages = {575--577},
volume = {25},
publisher = {Blackwell Publishing Ltd},
number = {3},
year = {1992},
title = {THE ORGANIZATION OF BEHAVIOR},
address = {Oxford, UK},
author = {Capaldi, E. John},
}



@article{PyramidalPop,
keywords = {Cerebral Cortex -- Physiology; Neurons -- Cytology; Pyramidal Tracts -- Physiology; Synapses -- Cytology; Synapses -- Physiology; Synapses -- Physiology; Synapses -- Ultrastructure;},
issn = {0301-0082},
journal = {Progress in Neurobiology},
pages = {563--607},
volume = {39},
publisher = {Elsevier Ltd},
number = {6},
year = {1992},
title = {The pyramidal neuron of the cerebral cortex: Morphological and chemical characteristics of the synaptic inputs},
language = {eng},
author = {Defelipe, Javier and Fariñas, Isabel},
}

@ARTICLE{10.3389/fncir.2016.00023,
  
AUTHOR={Hawkins, Jeff and Ahmad, Subutai},   
	 
TITLE={Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},      
	
JOURNAL={Frontiers in Neural Circuits},      
	
VOLUME={10},      

PAGES={23},     
	
YEAR={2016},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncir.2016.00023},       
	
DOI={10.3389/fncir.2016.00023},      
	
ISSN={1662-5110},   
   
ABSTRACT={Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}


@article{LeCunYann2015Dl,
keywords = {Computer Networks – Analysis ; Machine Learning – Analysis},
month = {5},
issn = {0028-0836},
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
journal = {Nature},
volume = {521},
publisher = {Nature Publishing Group},
number = {7553},
year = {2015},
title = {Deep learning.(Report)},
language = {eng},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
}


@misc{ANtikz,
  title = {Diagram of an artificial neural network},
  howpublished = {\url{https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network}},
  note = {Accessed: 2018-06-20}
}

@ARTICLE{10.3389/fncir.2017.00081,
  
AUTHOR={Hawkins, Jeff and Ahmad, Subutai and Cui, Yuwei},   
	 
TITLE={A Theory of How Columns in the Neocortex Enable Learning the Structure of the World},      
	
JOURNAL={Frontiers in Neural Circuits},      
	
VOLUME={11},      

PAGES={81},     
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/article/10.3389/fncir.2017.00081},       
	
DOI={10.3389/fncir.2017.00081},      
	
ISSN={1662-5110},   
   
ABSTRACT={Neocortical regions are organized into columns and layers.  Connections between layers run mostly perpendicular to the surface suggesting a columnar functional organization. Some layers have long-range excitatory lateral connections suggesting interactions between columns. Similar patterns of connectivity exist in all regions but their exact role remain a mystery. In this paper, we propose a network model composed of columns and layers that performs robust object learning and recognition. Each column integrates its changing input over time to learn complete predictive models of observed objects. Excitatory lateral connections across columns allow the network to more rapidly infer objects based on the partial knowledge of adjacent columns. Because columns integrate input over time and space, the network learns models of complex objects that extend well beyond the receptive field of individual cells. Our network model introduces a new feature to cortical columns. We propose that a representation of location relative to the object being sensed is calculated within the sub-granular layers of each column. The location signal is provided as an input to the network, where it is combined with sensory data. Our model contains two layers and one or more columns. Simulations show that using Hebbian-like learning rules small single-column networks can learn to recognize hundreds of objects, with each object containing tens of features. Multi-column networks recognize objects with significantly fewer movements of the sensory receptors. Given the ubiquity of columnar and laminar connectivity patterns throughout the neocortex, we propose that columns and regions have more powerful recognition and modeling capabilities than previously assumed.}
}

@article{DBLP:journals/corr/AhmadH15,
  author    = {Subutai Ahmad and
               Jeff Hawkins},
  title     = {Properties of Sparse Distributed Representations and their Application
               to Hierarchical Temporal Memory},
  journal   = {CoRR},
  volume    = {abs/1503.07469},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.07469},
  archivePrefix = {arXiv},
  eprint    = {1503.07469},
  timestamp = {Wed, 07 Jun 2017 14:42:49 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AhmadH15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/AhmadH16,
  author    = {Subutai Ahmad and
               Jeff Hawkins},
  title     = {How do neurons operate on sparse distributed representations? {A}
               mathematical theory of sparsity, neurons and active dendrites},
  journal   = {CoRR},
  volume    = {abs/1601.00720},
  year      = {2016},
  url       = {http://arxiv.org/abs/1601.00720},
  archivePrefix = {arXiv},
  eprint    = {1601.00720},
  timestamp = {Wed, 07 Jun 2017 14:41:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AhmadH16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}


@article{DBLP:journals/corr/Purdy16,
  author    = {Scott Purdy},
  title     = {Encoding Data for {HTM} Systems},
  journal   = {CoRR},
  volume    = {abs/1602.05925},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.05925},
  archivePrefix = {arXiv},
  eprint    = {1602.05925},
  timestamp = {Mon, 13 Aug 2018 16:49:03 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Purdy16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{7844355, 
author={Yuwei Wang and Yi Zeng and Bo Xu}, 
booktitle={2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)}, 
title={SHTM: A neocortex-inspired algorithm for one-shot text generation}, 
year={2016}, 
volume={}, 
number={}, 
pages={000898-000903}, 
keywords={one shot text generation;nature language processing;machine translation;question answering;deep learning;one shot learning;neocortex based computational model;semantic hierarchical temporal memory model;SHTM;Semantics;Brain modeling;Neurons;Pattern recognition;Biological system modeling;Computational modeling;Predictive models}, 
doi={10.1109/SMC.2016.7844355}, 
ISSN={}, 
month={10},}

@article{DBLP:journals/corr/Webber15,
  author    = {Francisco De Sousa Webber},
  title     = {Semantic Folding Theory And its Application in Semantic Fingerprinting},
  journal   = {CoRR},
  volume    = {abs/1511.08855},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08855},
  archivePrefix = {arXiv},
  eprint    = {1511.08855},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Webber15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Chen:2016:XST:2939672.2939785,
 author = {Chen, Tianqi and Guestrin, Carlos},
 title = {{XGBoost}: A Scalable Tree Boosting System},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '16},
 year = {2016},
 isbn = {978-1-4503-4232-2},
 location = {San Francisco, California, USA},
 pages = {785--794},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2939672.2939785},
 doi = {10.1145/2939672.2939785},
 acmid = {2939785},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {large-scale machine learning},
}


@book{Kurzweil:2013:CMS:2534455,
 author = {Kurzweil, Ray},
 title = {How to Create a Mind: The Secret of Human Thought Revealed},
 year = {2013},
 isbn = {9780143124047},
 publisher = {Penguin Books},
 address = {New York, NY, USA},
}



@article{ericssonCBA, 
    volume={93}, 
    url={https://www.ericsson.com/assets/local/publications/ericsson-technology-review/docs/2016/etr-telco-grade-paas.pdf}, 
    number={4}, 
    journal={Charting The Future Of Innovation Ericsson Technology Review}, author={Drake, Edvard and El Khayat, Ibtissam and Quinet, Raphaël and Wennmyr, Einar and Wu, Jacky}, 
    title = {Paving The Way To Telco-Grade PAAS},
    year={2016}, 
    month={5}
}

@book{cba,
abstract = {Component Oriented Programming offers a unique programming-centered approach to component-based software development that delivers the well-developed training and practices you need to successfully apply this cost-effective method. Following an overview of basic theories and methodologies, the authors provide a unified component infrastructure for building component software using JavaBeans, EJB, OSGi, CORBA, CCM, .NET, and Web services. You'll learn how to develop reusable software components; build a software system of pre-built software components; design and implement a component-based software system using various component-based approaches. Clear organization and self-testing features make Component Oriented Programming an ideal textbook for graduate and undergraduate courses in computer science, software engineering, or information technology as well as a valuable reference for industry professionals.},
isbn = {0471644463},
year = {2005},
title = {Component-oriented programming},
edition = {1},
language = {eng},
address = {S.l.]},
author = {Wang, Andy Ju An},
keywords = {Business &Amp; Economics ; Investments &Amp; Securities ; Options},
url = {http://portal.igpublish.com/iglibrary/search/WILEYB0014526.html},
lccn = {QA76.76.C66W36},
}

@book{QinZheng2008SA,
series = {Advanced Topics in Science and Technology in China},
abstract = {Part of the new series, Advanced Topics in Science and Technology in China, this book aims to introduce the theoretical foundations, various sub-fields, current research, and practical methods of software architecture. Readers can acquire basic knowledge of software architecture, including why software architecture is necessary, how we can describe a system’s architecture with formal language, what architecture styles are popular in practice, and how we can apply software architecture to the development of systems. Case studies, data, illustrations, and other materials released within the past 5 years will be used to show the latest developments in software architecture. Dr. Qin Zheng is doctoral mentor of the computer science and technology departments at Tsinghua and Xi’an Jiaotong Universities. He has been Associate Dean of the School of Software, Tsinghua University, and Chair of the Institute of E-commerce, Xi’an Jiaotong University. He has been a visiting scholar at several universities in the United States.},
isbn = {9786612005237},
year = {2008},
title = {Software Architecture},
language = {eng},
author = {Qin, Zheng},
keywords = {Computer science; Software engineering; Computer Science; Software Engineering/Programming and Operating Systems; Software Engineering; Electronic books},
}


@Inbook{memory,
author="Hole, Kjell J{\o}rgen",
title="The HTM Learning Algorithm",
bookTitle="Anti-fragile ICT Systems",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="113--124",
abstract="According to the fail fast principle in Chap.  4, we need to learn from systems' abnormal behavior and downright failures to achieve anti-fragility to classes of negative events. The earlier we can detect problems, the smaller the negative consequences are and the faster we can start learning how to improve the systems. Since humans are not good at detecting anomalies, especially in streaming data from large cloud applications, a form of automatic anomaly detection is needed. This first chapter of Part IV introduces a general learning algorithm based on Jeff Hawkins's developing theory of how the brain learns, called hierarchical temporal memory (HTM). The HTM learning algorithm is used in the next chapter to detect anomalies in a system's behavior.",
isbn="978-3-319-30070-2",
doi="10.1007/978-3-319-30070-2_11",
url="https://doi.org/10.1007/978-3-319-30070-2_11"
}



@ARTICLE{10.3389/fnana.2013.00051,
  
AUTHOR={Marín-Padilla, Miguel},   
	 
TITLE={The mammalian neocortex new pyramidal neuron: a new conception},      
	
JOURNAL={Frontiers in Neuroanatomy},      
	
VOLUME={7},      

PAGES={51},     
	
YEAR={2014},      
	  
URL={https://www.frontiersin.org/article/10.3389/fnana.2013.00051},       
	
DOI={10.3389/fnana.2013.00051},      
	
ISSN={1662-5129},   
   
ABSTRACT={Mammals’ new cerebral cortex (neocortex) and the new type of pyramidal neuron are mammalian innovations that have evolved for operating their increasing motor capabilities using essentially analogous anatomical and neural makeups. The human neocortex starts to develop in the 6-week-old embryo with the establishment of a primordial cortical organization that resembles the primitive cortices of amphibian and reptiles that operated his early motor activities. From the 8th to the 15th week of age, the new pyramidal neurons, of ependymal origin, are progressively incorporated within this primordial cortex forming a cellular plate that divide its components into those above it (neocortex first lamina) and those below it (neocortex subplate elements). From the 16th week of age to birth and postnatally, the new pyramidal neurons continue to elongate functionally their apical dendrite by adding synaptic membrane to incorporate the needed sensory information for operating the animal muscular activities. The new pyramidal neuron’ distinguishing feature is the capacity of elongating anatomically and functionally its apical dendrite (its main receptive surface) without losing its original attachment to first lamina or the location of its soma retaining its essential nature. The number of pyramidal cell functional strata established in the motor cortex increases and reflects each mammalian species motor capabilities: the hedgehog needs 2 pyramidal cell functional strata to carry out all its motor activities, the mouse three, cat four, primates 5 and humans 6. The presence of six pyramidal cell functional strata distinguish the human motor cortex from that of others primates. Homo sapiens represent a new evolutionary stage that have transformed his primate brain for operating his unique motor capabilities, such as speaking, writing, painting, sculpturing including thinking as a premotor activity.}
}

@book{Szyperski:2002:CSB:515228,
 author = {Szyperski, Clemens},
 title = {Component Software: Beyond Object-Oriented Programming},
 year = {2002},
 isbn = {0201745720},
 edition = {2nd},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
} 