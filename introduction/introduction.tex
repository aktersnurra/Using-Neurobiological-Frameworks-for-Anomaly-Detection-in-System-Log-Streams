\chapter{Introduction}
We start by giving an introduction to the purpose of this thesis and previous and related work. Then, we give the reason for using the entire data set for both training and testing. Finally, we briefly go over the main topics of each of the following chapters.


\section{Background}
Recent techniques in machine learning have shown impressive results when used on large scale data in order to find and classify patterns, most commonly in the field of image, speech and text classification \cite{image,speech,text}. At the Ericsson System \& Technology unit, a fault detection system has been developed that is able to predict faults from system logs generated by Ericsson's \textit{Component Based Architecture} (CBA) applications. The fault detection system uses traditional machine learning techniques of logistic regression trained with stochastic gradient descent \cite{linnaeus}. However, the System \& Technology unit wanted to explore new ways to improve their fault detection system by reimplementing it with a state-of-the-art machine learning algorithm called \textit{Hierarchical Temporal Memory} (HTM). HTM, unlike most machine learning algorithms, is a biologically derived algorithm from research done in neuroanatomy and neurophysiology. The intent of HTM theory is to replicate the information processing algorithm in the \textit{new cerebral cortex} (neocortex) \cite{bami}. The theory and framework behind HTM is open-source and has been developed by a company called Numenta.


The reason why Ericsson is looking to solve this classification problem with machine learning, is because the system logs are changing during the development of the CBA application, e.g. components are upgraded, or new ones are installed. Therefore, a basic template matching of system logs with known faults would not work, as Ericsson discovered when they investigated this before. Thus, there is a need for a machine learning application that is able to detect semantically similar logs, while at the same time being robust to changes. 


\section{Purpose and goals}
In this degree project we explore the possibility of anomaly detection in system logs with an HTM model. Therefore, the thesis main objective is to do a proof of concept, where we compare the HTM models performance with the existing model. The HTM model works by predicting a fault class for each system log it is fed. The reason for this model behaviour is to quickly go through system log files and detect if there are any faults or anomalies present. The model tries to solve the problem of the long lead times that exist today between fault, detection, and repair. If the model is reliable, it would give a technician the advantage of having a narrower search space when troubleshooting a faulty application.


\section{Related Work}
Linnaeus is a fault detection system that detects and classifies faults in Ericsson's CBA applications. The main reason for its development was to reduce the lead time from the moment a fault is detected until it is repaired. Previously, vast amounts of time was ``wasted'' by going through the system logs manually and passing them between engineering teams before the faulty component was identified and addressed. The intention of Linnaeus is to perform real-time classification and fault prediction on production systems. Linnaeus is beneficial in two ways; firstly, it raise an alarm if there is a potentially fault. Secondly, it could help a technician with information when writing a trouble report and sending it to the correct software component team straight away \cite{linnaeus}.


To classify data, Linnaeus uses supervised learning. Hence, a labeled training set is needed to train the parameters of the model. The raw data is acquired from CBA applications running in test environments and collected in a database \cite{linnaeus}. The data is fed through a data preparation pipeline where it is down sampled to only contain labeled examples of relevant logs of faults and non-faults. This is then saved as the training set \cite{linnaeus}.


To train the model, the training data is fed through a training pipeline, where the parameters of the machine learning model are optimised via a learning algorithm. The training data is transformed by only keeping whole words, and removing unnecessary specific data such as component names and time stamps. The words in the system log file are segmented either using uni-grams or bi-grams, thus individual words or  two consecutive words together can be captured. The importance of the uni- and bi-grams are extracted by using Term-Frequency Inverse Document Frequency, which transforms the data into a matrix, where each word is represented by a value of its importance. This input matrix is then used to train a logistic regression model, where the weights of the model are trained with stochastic gradient descent, and the optimal hyper-parameters are found via grid search \cite{linnaeus}. The classification model is then saved for later use in the classification pipeline, where real-time system log files are fed to the model for classification \cite{linnaeus}.


\section{Research Question}
Due to the proof of concept nature of the task of the degree project, we investigate research problems which are more comparative in nature. Therefore, we will in this thesis look at the advantages and disadvantages of using a HTM model for fault detection of system logs, and compare these findings with the machine learning model of the existing system.


\section{Limitations}
The data set was not split into a training and testing set. The entire data set was used in both training and testing of the model. This decision was made due the unequal distribution of examples in the data set, where 95.2\% of the total 14459 examples belongs to one class. To still get a sense of the capabilities of the HTM model, one-shot learning was used, which means that each system log was only presented once to the model during training.


\section{Outline}
The thesis is structured into the following sections; theory, method, results, and discussion and conclusion. In \autoref{chp_cba}, we give a brief introduction of component based architecture and system logs, to give context to the data set. Then, in \autoref{chp_htm}, we give a detailed explanation of the HTM algorithm. In \autoref{chp_nlp}, we introduce the encoding method of system logs into binary semantic input vectors. The preprocessing step of the data set, together with the HTM model is presented in \autoref{chp_method}. An evaluation of the HTM models performance and comparison with \textit{Linnaeus} is given in \autoref{chp_res}. In \autoref{chp_disc} we have the discussion and conclusion of the thesis. Finally, we have \autoref{chp_neocortex}, which aims to give an overview of the neuroscience of the neocortex, to better grasp HTM theory.