\section{HTM Neuron Model}
The HTM neuron model is an abstraction of the pyramidal neuron in the neocortex \cite{10.3389/fncir.2016.00023}. However, so is the point neuron, which is used in most \textit{Artificial Neural Networks} (ANNs); so how different are they? The point neuron, see \autoref{fig:pointNeuron}, summates all the inputs on its synapses, then passes this value through a non-linear activation function. If the output value is above a threshold, the neuron outputs the value of the activation function; otherwise it will output a zero. With the properties of the dendrites explained in \autoref{sec:dendrite}, one could argue that point neurons does not have dendrites at all, and therefore completely lack the active properties of the dendrites. 


The connection between the point neurons is instead the synaptic connection, which can be changed via back propagation \cite{LeCunYann2015Dl}. 

\begin{figure}[ht!]
    \centering
    \input{background/HTM/tikz/pointNeuron.tex}
    \caption{The point neuron, used in most ANNs, summates the synaptic input and passes in through an activation function. It lacks active dendrites and only has synaptic connections.} %\cite{ANtikz}
    \label{fig:pointNeuron}
\end{figure}


The HTM neuron, see \autoref{fig:HTMNeuron}, is modelled on the active dendrite properties, and is therefore able to make use of the coincidence detection of the pyramidal neuron. The coincidence detection is activated via non-linear integration when a small number of synapses, experimentally show to be around 8-20, are active in close spatial proximity on a dendritic segment \cite{10.3389/fncir.2016.00023}. This non-linear integration will cause an NMDA dendritic spike, thus allowing the neuron to recognise a pattern \cite{10.3389/fncir.2016.00023}. In order for the neuron to be able to recognise a vast number of different patterns, the active input pattern needs to be sparse, i.e. only a few neurons that are active per input pattern. If we assume that the total number of neurons in a population is $n$, and at any given time the number of active cells are $a$, then sparse activation is given as $a \ll n$. On each dendritic segment there are $s$ number of synapses. For a dendritic segment to release an NMDA spike, the number of synapses that needs to be active is $\theta$, i.e. the NMDA spike threshold, of the total number of synapses $s$ \cite{10.3389/fncir.2016.00023}. By forming more synaptic connections for each pattern than necessary the neuron becomes more robust to noise and variation in the input pattern. However, the trade-off in these extra connections is the increased likelihood of the neuron to classify false positives, but if the patterns are sparse the increased likelihood is infinitesimal \cite{10.3389/fncir.2016.00023}. The dendrites can be divided into three zones of synaptic integration, the \textit{basal}, \textit{proximal}, and \textit{distal zone} \cite{10.3389/fncir.2016.00023}. These zones are categorised based on input and spatial position on the neuron, and are explained below.

\begin{figure}[ht!]
    \centering
    \hbox{\hspace{3.5em}\input{background/HTM/tikz/HTMNeuron.tex}}
    \caption{The schematic of the HTM neuron with arrays of coincident detectors consisting sets of synapses. However, in this figure only a few is shown, where black dots represents active synapses and white inactive ones. An NMDA spike is generated if the total number of active synapses are above the NMDA spike threshold, $\theta$, (represented as a Heaviside node) on any of the coincident detectors in a dendritic zone, which is represented by OR-gate. The dendritic zones can be divided in to three different zones base on the distance from the soma and synaptic connections. The proximal dendrites receive the \textit{feedforward} pattern, also know as the receptive field of the neuron. The basal zone receives information about the activity of neighbouring neurons of which its connected to and can be seen as giving \textit{context} to the input pattern. Apical dendrites receive the \textit{feedback} information form the layers above which also can effect the state of the soma \cite{10.3389/fncir.2016.00023}.}
    \label{fig:HTMNeuron}
\end{figure}



\subsection{Proximal Zone}
The feedforward input is received by the dendrites in the proximal zone, as this is the main receptive field of HTM neuron \cite{10.3389/fncir.2016.00023}. The proximal zone is the dendritic zone closest to the soma, usually consisting of several hundreds of synapses. Because of the proximity to the soma, the NMDA spike generated in this dendritic zone is strong enough to effect the soma in such a way that it generates an action potential. If the input pattern is sparse, subsets of synapses are able to generate NMDA spikes. Therefore, the coincident detector can detect multiple different feedforward patterns in one input signal, thus it can be viewed as a union of several unique patterns \cite{10.3389/fncir.2016.00023}.


\subsection{Basal Zone}
The basal zone is the dendritic segment that connects neuron in different minicolumns to each other. These connection allow a neuron to detect activity of neighbouring neurons, which enables individual neurons to learn transitions of input patterns. When a basal segment recognises a pattern, it will generate an NMDA spike. But due to the distance from the soma, the signal attenuates and is not able to generate an action potential in the soma. However, it does depolarise the soma, also called the \textit{predictive state} of the neuron. The \textit{predictive state} is an important state of the neuron because it has major contribution to the overall network functionality. If a neuron is in the predictive state it will become active earlier than its neighbours, in the same minicolumn and close proximity, if the feedforward pattern activates the proximal segment. When a neuron transitions from the predictive state to the active state it will not only give of an action potential, but also inhibit its neighbours from becoming active. Thus, keeping the activation pattern for recognised input patterns sparse \cite{10.3389/fncir.2016.00023}. This type of inhibition of nearby neurons is a way to represent the functionality of inhibition neurons, without representing them as individual cells \cite{10.3389/fncir.2017.00081}.


\subsection{Distal Zone}
Furthest from the soma is the apical dendrites, which connects neurons to the ascending layers. Much like the basal dendrites, apical segment does not generate a signal strong enough to cause an action potential in the soma. The signal generated on the apical segment differs from the signal generated on the basal segment. When a pattern is recognised, the NMDA spike does not directly travel to the soma. Instead, the soma is depolarises by a calcium ion, Ca$^{2+}$, spike generated at the dendritic segment. This depolarisation gives the neuron a ability of doing top-down extraction \cite{10.3389/fncir.2016.00023}.


\subsection{Learning}
The learning of an individual HTM neuron is based on two principles; formation and removal of synaptic connection via Hebbian style learning \cite{10.3389/fncir.2016.00023}. Each dendritic branch has a set of potential synaptic connection, where each connection can become active if there is enough simultaneous activity between the two potentially connected neurons. For a dendritic branch to recognise a pattern there needs to be a subset of the connected synapses that are active. This threshold is usually set to 15-20 \cite{10.3389/fncir.2016.00023}. When a dendritic branch becomes active, the entire dendritic segment is seen as active to the neuron, which is visualised in \autoref{fig:HTMNeuron} by the OR gate. The HTM neuron learns to detect new pattern by forming new synaptic connection on a dendritic branch. Each potential synaptic connection is given a \textit{permanence} value, which determines the strength of a synaptic connection between the neuron's dendritic branch and another neuron's axon in the network. The permanence value is defined on the range $[0, 1]$, where $0.0$ means that there is no connection, and $1.0$ means that a fully formed synapses has been grown. The potentiation and depression of each permanence value is achieved via a Hebbian learning rule, i.e. if neurons fire together they wire together. In order for a synaptic connection to be formed, the permanence value has to be above a certain threshold, e.g. $0.3$. With a permanence value above the threshold, the synaptic weight is assigned to 1 between the neurons. Therefore, there is no difference if the permanence value is $0.3$ or $1.0$ when a pattern is recognised. However, the lower the value is, the easier it is for the neuron to forget the connection, and in extension the pattern. With this growing mechanism of the neuron, the tolerance to noise and on-line learning is possible \cite{10.3389/fncir.2016.00023}. 


